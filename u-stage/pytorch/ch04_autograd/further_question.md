# Further Question

## Autograd & Optimizer
- 1 epoch에서 이뤄지는 모델 학습 과정을 정리해보고 성능을 올리기 위해서 어떤 부분을 먼저 고려하면 좋을지 같이 논의해보세요
- `optimizer.zero_grad()`를 안하면 어떤 일이 일어날지 그리고 매 batch step마다 항상 필요한지 같이 논의해보세요

### Q1) 모델 학습 과정과 개선 방안 고민
학습 과정을 코드화시켜보면 아래와 같다
```python
1    for batch in dataloader:
2        X, y = prepare_inputs(batch)
3        y_pred = model(X)
4        loss = loss_fn(y_pred, y)
5        optimizer.zero_grad()
6        loss.backward()
7        optimizer.step()
```

- 우선, 1번 라인에서 데이터 자체를 증강시키거나 class imbalance 조율 등으로 데이터 퀄리티 자체에 집중하는 방안을 고려할 수 있다.
- 1번 라인에 추가로 large batch size를 줘서 성능 개선을 도모
- 3번 라인의 모델을 바꿔주거나 Mix-precision 혹은 ensemble 적용도 가능하다
- 4번 라인에서 loss에 label smoothing을 취해주거나 우리의 objective에 걸맞는 loss function을 개발하여 사용하는 것도 가능할 것 같다
- 7번 라인의 optimizer의 parameter를 조정하거나 다른 알고리즘을 사용하는 전략도 가능하다

### Q2) `optimizer.zero_grad()`는 꼭 필요한가?
아래 코드 예시를 보자
```python
import random
import numpy as np
import torch
import torch.nn as nn


# 실험 세팅
seed = 42
batch_size = 32
num_features = 10
num_classes = 3

# 난수 고정
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)

# 분류 문제에 필요한 입력값과 ground truth 제작
x = torch.randn(batch_size, num_features)
gt = torch.LongTensor(batch_size,).random_(num_classes)

# 간단한 MLP 모델 정의
model = nn.Sequential(
    nn.Linear(num_features, num_features),
    nn.ReLU(),
    nn.Linear(num_features, num_classes),
)
# loss 함수와 optimizer 정의
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

# 예측 및 loss 계산
y_pred = model(x)
loss = loss_fn(y_pred, gt)
loss
# tensor(1.1118, grad_fn=<NllLossBackward>)

# 아직은 backpropagation전! 때문에 현재는 gradient가 없다.
# 이 친구는 model[0].weight.grad와 동일하다
next(model.parameters()).grad
# None

# model[0].weight로 loss를 미분한 결과값을 가져온다
# 아래에서 이 그래프를 다시 이용할 것이기 때문에 retain_graph=True로
# 그래프를 유지해준다.
grad1 = torch.autograd.grad(loss, model[0].weight, retain_graph=True)
grad1[0]
# tensor([[-1.3665e-02, -1.8583e-02,  8.0944e-03,  1.7311e-02,  8.9696e-05,
#          -1.5166e-02, -8.2545e-03, -3.6081e-03, -3.5681e-03,  3.1789e-02],
#         [ 2.0060e-02,  1.2472e-02, -6.1917e-03, -1.2826e-02,  3.0649e-03,
#           2.1516e-02, -8.7458e-03,  2.4258e-03,  1.4990e-03,  1.1793e-03],
#         [-3.4242e-02, -1.3863e-02,  2.7209e-02, -5.7892e-03, -3.3123e-02,
#          -1.6006e-02, -1.6000e-02, -2.5553e-02, -2.2186e-02, -1.7676e-02],
#         [ 2.8013e-02, -1.6098e-02, -4.5723e-04, -1.6172e-02,  1.7764e-02,
#          -2.4326e-02,  1.9630e-02,  5.4104e-03,  5.0220e-02,  3.7244e-02],
#         [ 2.7887e-02,  2.6219e-02, -2.4892e-02, -2.4758e-02, -3.1945e-02,
#           2.4272e-02,  2.1577e-02, -9.2383e-05,  3.6435e-03, -7.1498e-03],
#         [-3.2932e-03,  8.8052e-03, -9.9945e-03, -1.2192e-02, -7.4998e-03,
#           1.3668e-02,  1.0861e-02,  3.3077e-03, -8.9183e-03, -1.1121e-02],
#         [-2.6218e-03,  2.7779e-02,  8.3599e-03,  1.7151e-02,  2.0167e-03,
#           4.6114e-02,  1.3548e-02, -1.6468e-03, -1.9198e-02, -5.4202e-02],
#         [-1.6335e-02, -1.4525e-02, -1.2934e-02, -1.1233e-02, -1.5649e-02,
#          -8.4997e-03, -3.0860e-03,  8.6893e-03,  8.3487e-03, -1.3035e-02],
#         [-1.3539e-02, -1.0313e-02,  2.0122e-02,  6.9890e-03, -3.7544e-03,
#          -1.8518e-02, -6.3127e-03, -6.9924e-03, -1.1844e-02, -1.2477e-03],
#         [-2.9210e-02, -6.0373e-03, -1.3277e-02, -2.8970e-03, -1.3406e-02,
#           3.6670e-02, -1.4225e-02, -1.7516e-02, -1.9731e-02, -4.8740e-02]])

# 역전파 실시!
loss.backward()
# 위에서 가져온 grad1의 값과 완벽하게 같다!
model[0].weight.grad
# tensor([[-1.3665e-02, -1.8583e-02,  8.0944e-03,  1.7311e-02,  8.9696e-05,
#          -1.5166e-02, -8.2545e-03, -3.6081e-03, -3.5681e-03,  3.1789e-02],
#         [ 2.0060e-02,  1.2472e-02, -6.1917e-03, -1.2826e-02,  3.0649e-03,
#           2.1516e-02, -8.7458e-03,  2.4258e-03,  1.4990e-03,  1.1793e-03],
#         [-3.4242e-02, -1.3863e-02,  2.7209e-02, -5.7892e-03, -3.3123e-02,
#          -1.6006e-02, -1.6000e-02, -2.5553e-02, -2.2186e-02, -1.7676e-02],
#         [ 2.8013e-02, -1.6098e-02, -4.5723e-04, -1.6172e-02,  1.7764e-02,
#          -2.4326e-02,  1.9630e-02,  5.4104e-03,  5.0220e-02,  3.7244e-02],
#         [ 2.7887e-02,  2.6219e-02, -2.4892e-02, -2.4758e-02, -3.1945e-02,
#           2.4272e-02,  2.1577e-02, -9.2383e-05,  3.6435e-03, -7.1498e-03],
#         [-3.2932e-03,  8.8052e-03, -9.9945e-03, -1.2192e-02, -7.4998e-03,
#           1.3668e-02,  1.0861e-02,  3.3077e-03, -8.9183e-03, -1.1121e-02],
#         [-2.6218e-03,  2.7779e-02,  8.3599e-03,  1.7151e-02,  2.0167e-03,
#           4.6114e-02,  1.3548e-02, -1.6468e-03, -1.9198e-02, -5.4202e-02],
#         [-1.6335e-02, -1.4525e-02, -1.2934e-02, -1.1233e-02, -1.5649e-02,
#          -8.4997e-03, -3.0860e-03,  8.6893e-03,  8.3487e-03, -1.3035e-02],
#         [-1.3539e-02, -1.0313e-02,  2.0122e-02,  6.9890e-03, -3.7544e-03,
#          -1.8518e-02, -6.3127e-03, -6.9924e-03, -1.1844e-02, -1.2477e-03],
#         [-2.9210e-02, -6.0373e-03, -1.3277e-02, -2.8970e-03, -1.3406e-02,
#           3.6670e-02, -1.4225e-02, -1.7516e-02, -1.9731e-02, -4.8740e-02]])

# optimizer로 gradient descent 실시!
optimizer.step()

# 여기서 다음 sample 값들 받고 원래는 zero_grad를 해줘야 한다
# 만일 하지 않았다면? 어떻게 되는지 아래를 확인하자

# 다음 샘플 받아오기
x = torch.randn(batch_size, num_features)
gt = torch.LongTensor(batch_size,).random_(num_classes)

# 예측 및 loss 계산
y_pred = model(x)
loss = loss_fn(y_pred, gt)
loss
# tensor(1.1516, grad_fn=<NllLossBackward>)

# model[0].weight로 loss를 미분한 결과값을 가져온다
# 아래에서 이 그래프를 다시 이용할 것이기 때문에 retain_graph=True로
# 그래프를 유지해준다.
grad2 = torch.autograd.grad(loss, model[0].weight, retain_graph=True)
grad2[0]
# tensor([[ 2.3985e-02, -2.1898e-03,  4.3209e-02,  1.4358e-02,  4.8940e-03,
#          -5.5665e-03,  1.9784e-02,  2.4296e-02,  1.7923e-02,  1.6746e-02],
#         [-2.1278e-02, -1.9172e-02,  9.2741e-05,  6.0226e-03,  1.6575e-02,
#          -6.4570e-03, -8.9712e-04,  1.6665e-02, -1.1201e-02,  7.8035e-03],
#         [ 2.3864e-02,  1.4301e-02, -1.6237e-02, -2.3799e-02,  1.4287e-02,
#          -6.5582e-03,  3.8327e-02, -3.8245e-03, -1.3729e-02, -1.0338e-02],
#         [ 1.7772e-02, -1.9806e-02,  1.8784e-02,  1.0511e-02,  6.2905e-02,
#          -6.1415e-04,  2.6416e-03,  2.0115e-02,  7.3462e-03,  2.4871e-02],
#         [-2.3495e-02, -1.6179e-02, -1.6960e-02, -1.8284e-02, -1.1401e-02,
#          -8.1893e-03, -2.2754e-02, -2.6657e-02, -1.5065e-02, -2.3589e-03],
#         [-4.5412e-03,  3.5421e-03, -2.2532e-02, -1.0486e-02, -1.3212e-02,
#           5.4097e-03, -1.6569e-02, -1.0346e-02, -1.3318e-02, -4.6968e-03],
#         [-8.9755e-03,  1.9964e-02, -2.1104e-02, -1.9879e-02, -5.6591e-02,
#          -7.9121e-03, -1.1880e-02, -4.6404e-02, -1.3953e-02, -4.6909e-02],
#         [-2.4859e-03,  2.6177e-02, -1.4391e-02, -8.5036e-03, -2.8412e-02,
#           8.0285e-03, -9.8991e-03, -2.0079e-02,  5.1347e-03, -1.4655e-02],
#         [ 8.6383e-03,  5.8688e-03, -6.3426e-03, -1.2480e-02, -5.9235e-03,
#           6.2074e-03, -1.3522e-03,  6.9045e-03,  8.4648e-03,  6.3736e-03],
#         [ 1.3832e-02,  9.9386e-03, -6.6953e-04, -2.1804e-02, -3.4901e-02,
#           2.0541e-02,  1.5276e-02, -9.4690e-03, -4.5095e-04, -1.8435e-02]])

# 역전파 실시!
loss.backward()
# 어라 근데 값이 이상하다?
model[0].weight.grad
# tensor([[ 1.0320e-02, -2.0773e-02,  5.1303e-02,  3.1669e-02,  4.9837e-03,
#          -2.0733e-02,  1.1529e-02,  2.0688e-02,  1.4355e-02,  4.8534e-02],
#         [-1.2171e-03, -6.7000e-03, -6.0989e-03, -6.8036e-03,  1.9640e-02,
#           1.5059e-02, -9.6429e-03,  1.9091e-02, -9.7020e-03,  8.9828e-03],
#         [-1.0378e-02,  4.3735e-04,  1.0972e-02, -2.9588e-02, -1.8837e-02,
#          -2.2565e-02,  2.2327e-02, -2.9377e-02, -3.5915e-02, -2.8014e-02],
#         [ 4.5785e-02, -3.5904e-02,  1.8327e-02, -5.6610e-03,  8.0668e-02,
#          -2.4940e-02,  2.2271e-02,  2.5525e-02,  5.7566e-02,  6.2115e-02],
#         [ 4.3917e-03,  1.0040e-02, -4.1852e-02, -4.3042e-02, -4.3347e-02,
#           1.6083e-02, -1.1768e-03, -2.6749e-02, -1.1422e-02, -9.5087e-03],
#         [-7.8344e-03,  1.2347e-02, -3.2526e-02, -2.2678e-02, -2.0712e-02,
#           1.9078e-02, -5.7086e-03, -7.0381e-03, -2.2236e-02, -1.5818e-02],
#         [-1.1597e-02,  4.7744e-02, -1.2744e-02, -2.7279e-03, -5.4575e-02,
#           3.8202e-02,  1.6678e-03, -4.8051e-02, -3.3151e-02, -1.0111e-01],
#         [-1.8821e-02,  1.1652e-02, -2.7325e-02, -1.9737e-02, -4.4061e-02,
#          -4.7124e-04, -1.2985e-02, -1.1390e-02,  1.3483e-02, -2.7690e-02],
#         [-4.9007e-03, -4.4441e-03,  1.3779e-02, -5.4905e-03, -9.6779e-03,
#          -1.2311e-02, -7.6648e-03, -8.7851e-05, -3.3789e-03,  5.1259e-03],
#         [-1.5377e-02,  3.9013e-03, -1.3947e-02, -2.4701e-02, -4.8307e-02,
#           5.7212e-02,  1.0513e-03, -2.6985e-02, -2.0182e-02, -6.7176e-02]])

# 왜 다른걸까? 아래 결과를 한 번 보자
# 아래는 model[0].weight.grad와
# 처음 gradient 그리고 이번에 전파된 gradient의 합이 같은지 확인하는 코드이다.
torch.isclose(model[0].weight.grad, grad1[0] + grad2[0]).all().item()
# True
```
- 누적 gradient값이 필요한 경우가 아니라면 이는 exploding gradient 문제를 야기시킬 것이다.
- 때문에 batch 시작마다 혹은 backward 전에 `zero_grad`를 해줄 것을 권장한다.
